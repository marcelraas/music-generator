{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch detection\n",
    "\n",
    "* Next case is to reconstruct which note was being played from the raw signal\n",
    "* Experimental setup:\n",
    "    1. Generate a sequence of notes using the music generator\n",
    "    2. Render it to a wave using the synthesizer\n",
    "    3. Chop the wave data into small batches of a 1/10 of a second\n",
    "    4. Calculate frequency features using Fourier analysis on these batches\n",
    "    5. For each of those batches, predict which note is being played\n",
    "        * Time resolution is equal to the batch size, we might have overlaps between notes\n",
    "    6. Treat it as a multi-class classification problem\n",
    "        * Each class corresponds to a note being played, multiple notes can be played together\n",
    "    7. Train a model with a Gated Recurrent Unit as classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does a Gated Recurrent Unit (GRU) work?\n",
    "\n",
    "Before we can explain a GRU we need to understand an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **recurrent** neural network. Normal feed forward neural networks predict only based on an input vector $x$, but in an RNN there is a hidden state that can be updated by $x$\n",
    "\n",
    "* This is particularly useful for time-series where the data can have **arbitrary length**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Output vector $o_t$ at timestep $t$,\n",
    "* input vector $x_t$ at timestep $t$,\n",
    "* Hidden state vector $h$ at timestep $t$,\n",
    "* The matrices $U$, $V$ and $W$ are subject to optimization (learning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the dimensionality of the hidden state we have more decision to make:\n",
    "* How much samples do we put in state vector $x$, just one? or a batch of samples?\n",
    "* How large are we going to make our batch? We can only update our weights after running one full batch.\n",
    "* What are we going to do with incomplete batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras the input to an RNN should be of rank 3:\n",
    "* `batch_size`: number of samples per batch\n",
    "* `timesteps`: number of samples\n",
    "* `input_dim`: dimensionality of the input vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible ways of designing our experiments\n",
    "\n",
    "* $x$ is one sample, we train the GRU on batches of ~4000 timesteps (0.1 secs) and number of `input_dim` = 1\n",
    "* $x$ is STFT transformed, `input_dim` = STFT dimensionality, we have only a couple of timesteps\n",
    "* In `Keras` there is the parameter `return_sequences` when instantiating a RNN layer. Default is set to `false`, however, when set to true it corresponds to the picture above, where for every timestep $t$ there is an output prediction $o_t$.\n",
    "* In `Keras` we can also set `stateful=True`. This will not reset the hidden state until the end of the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gru.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GRU is an RNN with the following components:\n",
    "* A hidden state: this is the recurrent vector, i.e. memory\n",
    "* A reset operation, `r[t]`, with a sigmoid (sigmoid has values between 0 and 1). Implemented as element-wise multiplication.\n",
    "* An update of the _proposed_ hidden state, `h[t]` via the `tanh` branch\n",
    "* A set operation `z[t]`. Also here, the sigmoid with values between 0 and 1 acts as a element-wise decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set operation in detail:\n",
    "\n",
    "$$(1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\hat{h}_t$$ \n",
    "\n",
    "which is \n",
    "\n",
    "* $h_{t-1}$ for $z = 0$, i.e. keep old state\n",
    "* $\\hat{h}_{t}$ for $z = 1$, i.e. switch to proposed state\n",
    "\n",
    "N.B. In the formula the symbols represent vectors and the product operator operates element-wise.\n",
    "For every vector component in the hidden state there can be a set or keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup: sigmoid & hyperbolic tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with tf.Session():\n",
    "    x = tf.range(-10, 10, delta=0.1)\n",
    "    sigmoid = tf.sigmoid(x)\n",
    "    tanh = tf.tanh(x)\n",
    "    plt.plot(x.eval(), sigmoid.eval(), label='sigmoid')\n",
    "    plt.plot(x.eval(), tanh.eval(), label='tanh')    \n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
